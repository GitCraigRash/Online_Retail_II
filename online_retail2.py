# -*- coding: utf-8 -*-
"""Online_Retail2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aGT1ihkurJYxG8cUQykZtIIV8h9bVGd3

#Using online retail data to
#MAXIMIZE PROFITS.

This dataset contains two years of online retail sales data. It includes Invoice, InvoiceDate, Customer ID, StockCode, Quantity, Price, Description, and Country.

Rual, Jordon, Brian, and Craig (me) have taken this data from Kaggle to showcase our skills in Data Science. We consulted a senior data scientist for project direction as we needed to know how to benefit stakeholders.

We used various methods to generate valuble information for stakeholders.

###Method 1: Find Priority Countries by Total Spent.(DONE, may needs updating)

###Method 2: Code Customers by High, Meduim and Low 'Cart Value'(the total sum of all products purchased during an order process.)(WORKING)

###Method 3: Bar Chart of Product Trends By Month (WORKING)

###Method 4: Predict Repeat Customers using Random Forest. (DONE, Needs Optimizing)

###Method 5: Bar Graph of when Repeat Customers come. Bar graph of when New Customers come. [Jordon](TODO)

### Method 6: Multiple Linear Regression Model [Raul](DONE)

### Method 7: Ridge Regression [Brian]

Method 7 TODO What is the new results using mean_absolute_error with new featres?

### TODO: find a way to reduce excessive loading time for .csv

### TODO: Make code PEP8 complient.

#Download Libraries and Dataset
"""

!pip install category_encoders

# Import various libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from category_encoders import OrdinalEncoder

from sklearn.linear_model import Ridge

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Import online_retail_II.csv
from google.colab import files
uploaded = files.upload()

def wrangle(csv):
  '''
  Seperate Known from Unknown customers. Drop Discription and Invoice columns.

  '''
  # Set InvoiceDate as Index
  df = pd.read_csv(csv,
                 index_col="InvoiceDate",
                 parse_dates=True)
  # Dropped "Description" because 'StockCode' exists.
  # and is better for machine learning.
  df.drop(columns=["Description","Invoice"], inplace=True)

  # A large portion of 'Customer ID' values are null.
  # These customers will be seperated for analysis.
  # Created a boolean mask indicating which rows have missing values.
  missing_mask = df.isnull().any(axis=1)

  # Define the categorical features
  df['StockCode'] = OrdinalEncoder().fit_transform(df['StockCode'])
  df['Country'] = OrdinalEncoder().fit_transform(df['Country'])

  # Create 'TotalSpending' column to track cart value.
  df['TotalSpending'] = df['Quantity'] * df['Price']

  # Some countries have negative cart value.
  # We conclude that these entries are returns or losses of some kind.
  # We will exclude them from our study because the frequency
  # and cost of these transactions are trivial(TODO:Provide precentage of return cost to revenue).
  #df = df.loc[df["TotalSpending"]>0].reset_index(drop=True)

  # Use the mask to create two separate DataFrames.
  unknown_df = df[missing_mask]
  known_df = df[~missing_mask]
  return df, unknown_df, known_df

df, unknown_df, known_df = wrangle("/content/online_retail_II.csv")

"""##Three datasets:

##df includes all customers.

##known_df includes only known custmers.

##unknown_df includes only unknown customers.

Predict the quantity of the MOST common item, based on orders of the past 30 days.
step 1 filter by most common item
step 2 Feature engineer

# add a rolling function of the average of price variates over the last thirty days.
"""

df['StockCode'] = OrdinalEncoder().fit_transform(df['StockCode'])
df['Country'] = OrdinalEncoder().fit_transform(df['Country'])

type(df["InvoiceDate"][0])

minidf = df.head(5000)
minidf.head(100)

import Tensorflow
lstm_model = tf.keras.models.Sequential([
    # Shape [batch, time, features] => [batch, time, lstm_units]
    tf.keras.layers.LSTM(32, return_sequences=True),
    # Shape => [batch, time, features]
    tf.keras.layers.Dense(units=1)
])

"""##Method 1: Find Priority Countries by Total Spent.(Done, may need updating)"""

#Create list of countries and group by sum of total spending.
unique_countries = df["Country"].unique()
results = []
for i in unique_countries:
  count_spend = df['TotalSpending'].loc[df['Country']==i]
  results.append(sum(count_spend))

countries_by_spending = df.groupby("Country")["TotalSpending"].sum().reset_index()\
             .sort_values(by="TotalSpending",ascending=False).reset_index(drop=True)
pd.DataFrame(countries_by_spending)
display(countries_by_spending)

Rankings=list(zip(results,unique_countries))
a=pd.DataFrame(Rankings,columns=["total_spending","countries"])

a.sort_values("total_spending",ascending=False).reset_index(drop=True)

"""##Method 2: Code Known Customers by High, Meduim and Low Cart Value(the total sum of all products purchased during an order process.(WORKING)"""



"""1)Determine your customer segmentation goals and variables.

2)Break goals into customer-centric segmentation projects.

3)Set up and prioritize each customer segmentation project.

4)Collect and organize your customer data.

5)Segment your customers into groups of your choice.

6)Target and market to your client and user segments.

7)Run regular customer segmentation analysis.

###DataFrame of Known Customers
"""

df['TotalSpending'] = df['Quantity'] * df['Price']

Kndf=df.dropna(subset=["Customer ID"]).reset_index()

"""Create Customer Total Spending Column."""

customer_spending=Kndf.groupby("Customer ID")["TotalSpending"].sum().reset_index().sort_values(by="TotalSpending",ascending=False)
customer_spending["index"]=np.arange(0,len(customer_spending))
customer_spending.set_index("index",inplace=True)

fig = plt.figure(figsize = (10, 5))
x=range(len(customer_spending))
y=np.array(customer_spending["TotalSpending"])
y.shape
# creating the bar plot
plt.plot(x,y)

fig = plt.figure(figsize = (20, 2))
plt.boxplot(y,vert=False)

cs_sum=customer_spending["TotalSpending"].sum()

16648292.388*.10,16648292.388*.01

high_paying_customers=[]
start_sum=16648292.388
for r in range(len(customer_spending)):
  if start_sum > 16648292.388*.10:
    start_sum=start_sum-customer_spending["TotalSpending"][r]
    #high_paying_customers.append(customer_spending["Customer ID"][r])
    continue
  high_paying_customers=customer_spending["Customer ID"][:r]
  break
high_paying_customers

customer_spending[len(high_paying_customers)-1:len(high_paying_customers)+1]

remv_high_customers=customer_spending[len(high_paying_customers):]
remv_high_customers["index"]=np.arange(0,len(remv_high_customers))
remv_high_customers.set_index("index",inplace=True)

type(remv_high_customers["TotalSpending"].loc[1710])

med_paying_customers=[]
start_sum=16648292.388*.1
for row in range(len(remv_high_customers)):
  if start_sum > 16648292.388*.01:
    start_sum=start_sum-remv_high_customers["TotalSpending"][row]
    #med_paying_customers.append(customer_spending["Customer ID"][r])
    continue
  med_paying_customers=remv_high_customers["Customer ID"][:row]
  break
med_paying_customers

#test if medium and high paying customers do not overlap.
overlap = med_paying_customers.isin(high_paying_customers).all()
overlap

#Finally, change df to include a column called spending score and change all rows according to customer division.

df["spendingscore"]=np.nan
df

# check if element exists in series
if 14492.0 in med_paying_customers:
  print("yes")

for i in range(len(df)):
  x=high_paying_customers.count(df["Customer ID"][i])
  y=med_paying_customers.count(df["Customer ID"][i])
  if df["Customer ID"][i] == high_paying_customers
    df["spendingscore"][i]=1
    continue
  if y>0:
    df["spendingscore"][i]=2
    continue
  df["spendingscore"][i]=3

df.loc[df["Customer ID"]==14646.0]

#Second attempt at assigning labels to empty spendingscore column.
#reseting spendingscore column
df["spendingscore"]=np.nan
df

df["Customer ID"].unique()

if high_paying_customers.count(14646.0)>0:
  print("two")

x=df["spendingscore"].loc[df["Customer ID"]==13078.]
for i in range(0):
  if high_paying_customers.count(14646.0)>0:
    print("found in 1")
    x.fillna(1,inplace=True)
    continue
  if med_paying_customers.count(14646.0)>0:
    print("found in 2")
    x.fillna(2,inplace=True)
    continue
x

# Alternative assigment function using customer ID as bases for loop and loc as search and fillna() to fill rows.
for i in df["Customer ID"].unique():
  x=df["spendingscore"].loc[df["Customer ID"]==i]
  if high_paying_customers.count(i)>0:
    x.fillna(1,inplace=True)
    continue
  if med_paying_customers.count(i)>0:
    x.fillna(2,inplace=True)
    continue
  x.fillna(3,inplace=True)

df["spendingscore"].value_counts()

df.head()

df['spendingscore'].value_counts()

len(med_paying_customers),len(customer_spending)

High= 90%
Medium=9%
Low=<1%



x=Kndf["TotalSpending"].sum()
high_customers=[]
for i in range(len(Kndf)):
  x=x-customer_spending["TotalSpending"][i]
  if x<=customer_spending["TotalSpending"].sum()/10:
    break
  high_customers.append(Kndf["Customer ID"][i])

test=np.array(customer_spending['TotalSpending'])

High=np.percentile(test, 90)

High

mask=customer_spending["TotalSpending"]>5595
High=customer_spending.loc[mask]
Low=customer_spending.loc[~mask]

Kndf["high_value"]=''

np.where(Kndf["TotalSpending"]>5595, Kndf["high_value"]==1, Kndf["high_value"]==0)

Kndf

Kndf['high_value'] = np.where(
    [Kndf['Customer ID'] in list(High["Customer ID"])] , 1,0)
Kndf["high_value"]

High["Customer ID"][0]

High_Value=[]
for i in range(len(Kndf)):
  if Kndf['Customer ID'][i] in list(High["Customer ID"]) :
    High_Value.append(1)
  High_Value.append(0)
Kndf["high_value"]=High_Value

High_Value

"""#Method 3  Bar Chart of Product Trends By Month (WORKING)

"""

#There are over 5699 unique items. However, which items sell the most?
#We are going to create a dataframe that ranks items by volume.

df, unknown_df, known_df = wrangle("/content/online_retail_II.csv")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima_model import ARIMA

# Load the data
data = pd.read_csv('online_retail_II.csv', parse_dates=['InvoiceDate'])

# Aggregate the data by month
monthly_data = data.groupby(pd.Grouper(key='InvoiceDate', freq='M'))['Quantity'].sum().reset_index()

# Visualize the data
plt.plot(monthly_data['InvoiceDate'], monthly_data['Quantity'])
plt.xlabel('Month')
plt.ylabel('Number of items sold')
plt.show()

len(df["StockCode"].value_counts())

plt.figure(figsize=(10,10))
plt.hist(df["StockCode"].value_counts(), bins=200)
plt.show()

codes=df["StockCode"].unique()
type(codes.tolist())

#top_items=['84077', '85123A', '85099B', '21212', '84879', '22197', '17003', '21977', '84991', '22492', '15036', '21213', '22616', '84755', '20725', '22178', '22386', '21232', '84568', '84946', '21915', '22086', '85099F', '84992', '22952', '71459', '22355', '21982', '20724', '21975', '21980', '21080', '85099C', '23084', '16014', '21931', '21731', '22469', '21790', '22411', '47566', '22151', '21498', '84970S', '21981', '20727', '22423', '21985', '21984', '37410', '22383', '21929', '21181', '20719', '84212', '21928', '20728', '84945', '22417', '20726', '22629', '84978', '20668', '22384', '22382', '84836', '23203', '21137', '21166', '72741', '22356', '21094', '21231', '84692', '20972', '22910', '20971', '20713', '20914', '20723', '21326', '22189', '21175', '22630', '22470', '22585', '84270', '22029', '22693', '22961', '22489', '21497', '84970L', '21791', '21733', '20712', '22077', '22909', '84598', '20975', '22294', '22385', '82482', '79321', '21704', '22659', '16156S', '22090', '85152', '84947', '21500', '22379', '21086', '21121', '22457', '22326', '22551', '21967', '22865', '22560', '21499', '22969', '21210', '21976', '82494L', '21787', '21703', '22045', '21918', '84826', '23199', '22558', '22328', '22577', '40016', '21085', '22297', '21091', '22556', '21930', '22554', '22866', '21986', '22083', '20718', '22867', '23209', '21099', '84347', '21914', '82580', '75049L', '21889', '16161P', '20974', '21877', '22534', '22699', '21092', '23201', '22610', '22740', '21122', '21829', '22595', '22993', '71477', '22188', '85175', '22418', '21495', '20711', '22348', '82600', '82583', '22998', '22114', '22030', '16033', '22149', '22578', '22147', '17084R', '23206', '21891', '23310', '23307', '22555', '84949', '84029E', '84378', '21314', '35961', '22112', '22561', '84988', '21506', '22027', '22632', '21672', '82484', '22951', '21390', '21916', '21385', '20993', '16237', '22444', '21871', '23202', '22666', '22741', '21669', '22557', '20676', '15056N', '23077', '21668', '22467', '84380', '22139', '84997D', '22530', '84050', '21673', '21136', '48138', 'POST', '22536', '20973', '23344', '21124', '22662', '22111', '16161U', '22652', '21108', '22296', '22697', '20685', '21908', '21174', '79000', '22791', '62018', '15056BL', '22727', '22988', '21088', '22544', '22633', '85220', '21238', '22381', '21156', '47591D', '22734', '22759', '84997B', '21671', '22219', '22959', '22646', '22158', '21165', '21868', '22960', '21096', '22333', '15034', '22704', '21509', '21169', '21670', '22065', '23301', '22621', '22295', '37340', '22084', '22440', '23232', '23298', '22907', '22130', '23293', '22966', '21098', '22620', '22531', '21559', '22661', '21507', '35970', '22087', '22335', '23309', '85150', '22367', '23343', '84987', '23076', '21917', '22047', '84997C', '22553', '21989', '21154', '82581', '23230', '84950', '22698', '20677', '21485', '22726', '21892', '48194', '20979', '22041', '21754', '10002', '23204', '22154', '84375', '22464', '21481', '22432', '22771', '21936', '22950', '21786', '22243', '21240', '84029G', '21172', '22198', '22082', '21239', '23207', '48187', '84520B', '22834', '22491', '22816', '22712', '21519', '21479', '22720', '21988', '23231', '22150', '21623', '22579', '22352', '22037', '21523', '22710', '21935', '21242', '22419', '23200', '22609', '22945', '20991', '21900', '21078', '21810', '22488', '84832', '22413', '21974', '47503E', '21508', '21875', '23078', '22161', '21899', '22983', '21313', '72008', '22549', '23300', '82552', '22196', '84406B', '22602', '22064', '22437', '51014A', '22818', '20675', '21535', '85049E', '21874', '21621', '85110', '22138', '21155', '21429', '22046', '22596', '20996', '20983', '22728', '22631', '85049A', '22540', '21870', '22028', '37413', '23208', '23245', '21833', '84596B', '22713', '20992', '20717', '21983', '21811', '47556B', '17096', '18007', '85014B', '22152', '22900', '23308', '23170', '23171', '21901', '22155', '21987', '85199S', '84971S', '23295', '22227', '48185', '82582', '22156', '21544', '37370', '22338', '21164', '85061W', '21830', '22563', '22571', '21592', '22113', '22722', '21746', '22273', '22537', '21035', '22466', '20984', '22986', '22528', '22645', '20679', '47599A', '22663', '22153', '22614', '22567', '22539', '23355', '22608', '21755', '21114', '72351B', '22507', '23205', '22908', '21243', '21876', '47590B', '47503A', '20759', '21843', '20756', '22593', '82578', '16046', '22601', '21844', '20760', '37351', '16047', '22144', '22174', '22024', '22711', '22708', '82551', '21524', '85178', '23167', '22716', '22570', '22569']

#Takes 4m to compute.
codes=df["StockCode"].unique()
codes.tolist()
quantities=[]
for i in codes[:-1]:
  x=df['Quantity'].loc[df["StockCode"]==i]
  x=sum(x)
  quantities.append(x)
  items=list(zip(codes,quantities))
items=pd.DataFrame(top_items,columns=["StockCode","Number Ordered"])
items

top_items=top_items.sort_values(by="Number Ordered",ascending=False).head(500)

df.groupby(pd.Grouper(key='InvoceDate', freq='M'))

x=pd.to_datetime("2009-12-01 07:45:00")
x_list=(x,x,x,x)
b=x_list.groupby(pd.Grouper(key='InvoceDate', freq='M'))
b

for i in top_items["StockCode"]:
  x=df.loc[df['StockCode']==i]
    for k in range(len(x)):
      if x[k].month==1
        n.append(1)

plt.figure(figsize=(10,10))
plt.hist(top_items.value_counts(), bins=200)
plt.show()

top_items.sorted(ascending=False).head(500)

df['StockCode'].loc[df["StockCode"].value_counts()>500)]

meth2_df=df[['InvoiceDate','StockCode','Quantity']]
meth2_df.set_index("InvoiceDate",inplace=True)
meth2_df.head()

type(meth2_df['InvoiceDate'][0])

meth2_df["Month"]=pd.DatetimeIndex(df["InvoiceDate"]).month
meth2_df["Month"].value_counts()

meth2_df.loc[df['StockCode']contains ]

stockseries=meth2_df['StockCode']

months=[12,1,2,3,4,5,6,7,8,9,10,11,12,1,2,3,4,5,6,7,8,9,10,11,12]
for i in months:

  for i in stockseries:
    rows=df.loc[df['StockCode']==i]
    date=.to_datetime64()
  date=np.datetime64(date, 'M')
  date + 1
#every step another month is added until the last month is reached.
# The totals are then saved as values to a dictionary with Stockseries.index as the keys.

months = ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
          "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]
df['months'] = pd.Categorical(df['months'], categories=months, ordered=True)
df.sort_values(...)  # same as you have now; can use inplace=True

"""###Deal with Null Values"""



#Found all Missing Customers
MissingCustomers=df.loc[df['Customer ID'].isnull()==True]
#Distribution of Quantiy Ordered for all Customers
fig = plt.subplots(figsize =(12, 8))
plt.hist(df['Quantity'], bins=100,range=(-20,50))

plt.show()

#Distribution of Quantiy Ordered for all Customers
fig = plt.subplots(figsize =(12, 8))
plt.hist(MissingCustomers['Quantity'], bins=100,range=(-20,50))

plt.show()

known_customers=df.loc[df['Customer ID'].isnull()==False]

len(known_customers['Customer ID'].loc[known_customers['Customer ID']==12748.0])

known_customers['Customer ID'].value_counts().index()

NonR_customer=[]
for i in df['Customer ID'].value_counts().index:
  if len(known_customers['Customer ID'].loc[known_customers['Customer ID']==i])>1:
    continue
  NonR_customer.append(i)

"""##Method 3:Find Repeat Customers and Perform Random Forest Classifier"""

Repeat_customer=[]
for i in df['Customer ID']:
  if i in NonR_customer:
    Repeat_customer.append(False)
    continue
  Repeat_customer.append(True)

df['Repeat_Customer']=Repeat_customer
df.columns

"""###TODO:Replace Country with Ordinal Encoder
###TODO:Standardize the values(price, quantitiy)
"""

#Predict Repeat Customers Using ____  'StockCode', 'Quantity', 'InvoiceDate', 'Price',  'Country',
X=df[['StockCode', 'Quantity', 'InvoiceDate', 'Price',  'Country']]
y=df['Repeat_Customer']

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20)

len(X_test)/len(X_train)

#Encode StockCode
from sklearn.preprocessing import OrdinalEncoder
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier

Pipe=make_pipeline(OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),RandomForestClassifier())
Pipe.fit(X_train,y_train)
#df['StockCode']=Ord.transform(df['StockCode'])
#df['Country']=Ord.tranform(df['Country'])

print(Pipe.score(X_test,y_test))

from sklearn.metrics import f1_score

f1_score(y_test,Pipe.predict(X_test))

from sklearn.metrics import classification_report

print(classification_report(y_test,Pipe.predict(X_test)))

"""###From the data, we can see that a grand majority of customers are returning customers. Churn is extremly low."""



"""#Method 6: Multiple Linear Regression"""

!pip install pystan~=2.14
!pip install fbprophet

from fbprophet import Prophet

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Set the date column as the index of the dataframe
df.set_index('date', inplace=True)

# Decompose the time series into trend, seasonality, and residuals using statsmodels
decomposition = sm.tsa.seasonal_decompose(df, model='additive')

# Plot the original time series and the decomposed components
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 8))
ax1.plot(df)
ax1.set_title('Original Time Series')
ax2.plot(decomposition.trend)
ax2.set_title('Trend')
ax3.plot(decomposition.seasonal)
ax3.set_title('Seasonality')
ax4.plot(decomposition.resid)
ax4.set_title('Residuals')

# Show the plot
plt.show()



#LEAST ORDINARY SQUARES
# Libraries for R^2 visualization
from ipywidgets import interactive, IntSlider, FloatSlider
from math import floor, ceil
from sklearn.base import BaseEstimator, RegressorMixin

# Libraries for model building
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

from category_encoders import OrdinalEncoder

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

target ='Quantity'
X = known_df.drop(columns = target, axis = 1)
y = known_df[target]

X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2,random_state=42)

# Providing the mean of our target we can have a baseline we always work with our train data & leave the test data for the end.
y_pred_baseline = [y_train.mean()] * len(y_train)
baseline_mae = mean_absolute_error(y_train, y_pred_baseline)

print('Quantity:', y_train.mean())
print('Baseline MAE:', baseline_mae)

#Fitting
model=LinearRegression()
model.fit(X_train,y_train)

#MAE Accuracy Score
print('Training MAE:', mean_absolute_error(y_train, model.predict(X_train)))
print('Test MAE:', mean_absolute_error(y_test, model.predict(X_test)))

# Training RMSE
print('Training R-Squared:', r2_score(y_train, model.predict(X_train)))

# Test RMSE
print('Test R-Squared:', r2_score(y_test, model.predict(X_test)))

# Coefficient with the most value allways is the absolut value of our coefficient in this case is the [0]
print('The coefficient for Stock Code is:', model.coef_[0])
print('The coefficient for Price is:', model.coef_[1])
print('The coefficient for Customer ID is:', model.coef_[2])
print('The coefficient for Country is:', model.coef_[3])
# Intercept
#print('The intercep for our model is:', model.intercept_)

"""#Ridge Regression"""

known_df["avg_price_30d"]=known_df["Price"].rolling(window=30).mean()

known_df["avg_price_90d"]=known_df["Price"].rolling(window=90).mean()

known_df["avg_price_1y"]=known_df["Price"].rolling(window=252).mean()
known_df.dropna(inplace=True)

known_df["avg_quant_30d"]=known_df["Quantity"].rolling(window=30).std()

known_df["avg_quant_90d"]=known_df["Quantity"].rolling(window=90).std()

known_df["avg_quant_1y"]=known_df["Quantity"].rolling(window=252).std()
known_df.dropna(inplace=True)

known_df

#Ridge Regression

target ='Quantity'
X = known_df.drop(columns = target, axis = 1)
y = known_df[target]

X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2,random_state=42)

X

# Providing the mean of our target we can have a baseline we always work with our train data & leave the test data for the end.
y_pred_baseline = [y_train.mean()] * len(y_train)
baseline_mae = mean_absolute_error(y_train, y_pred_baseline)
baseline_mae

y_train

from sklearn.model_selection import GridSearchCV

#Fitting
ridge=Ridge()
parameters={'alpha':[1,.5,.25,.10,.001]}
grid=GridSearchCV(ridge,parameters,n_jobs=-1,scoring='neg_mean_absolute_error')


grid.fit(X_train,y_train)

from sklearn.metrics import get_scorer_names
get_scorer_names()

grid.best_params_

grid.best_score_,baseline_mae



a=grid.best_estimator_.coef_
for i,k in enumerate(a):
  print(round(i,9),k)

X

y_pred=grid.predict(X_test)

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
import numpy as np

mae = mean_absolute_error(y_test, y_pred)

# Get coefficients
coefficients = gird.coef_

print("Mean Absolute Error:", mae)
print("Coefficients:", coefficients)

#This model is a wierd predictor.

