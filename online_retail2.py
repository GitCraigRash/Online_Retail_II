# -*- coding: utf-8 -*-
"""Online_Retail2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NNSMOeC3GRkCQMr0xk7kbHxNcEyfcdVo

#Using online retail data to
#MAXIMIZE PROFITS.

###Method 1: Find Priority Countries by Total Spent.(DONE, may need updating)

###Method 2: Code Customers by High, Meduim and Low 'Cart Value' (sum of all products purchased during an order process.)(DONE could need optimizing)[Raul]

###Method 3: Bar Chart of Product Trends By Month (WORKING)

###Method 4: Predict Repeat Customers using Random Forest. (DONE, Needs Optimizing)

###Method 5: Bar Graph of when Repeat Customers come. Bar graph of when New Customers come. [Jordon](TODO)

### Method 6: Multiple Linear Regression Model [Raul](DONE)

### Method 7: Ridge Regression[Brian?]

Method 7 TODO What is the new results using mean_absolute_error with new featres?

##Step1) Create Hypothosis

Find seasonal products/trends

TODO create graph of repeat customers etc. Jan -March , Oct-Dec

When do new customers come in?(when to do marketing campaigns)

Increase cart value(avg. how much customer spent across the month). Which product/month has highest cart value.

Spending Score based on ??? Bucket (low,med,high) value. Look at average price.
##Step 2) Go to the data
"""

!pip install category_encoders

#Import various libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from category_encoders import OrdinalEncoder

from sklearn.linear_model import Ridge

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from google.colab import drive
drive.mount('/content/drive')
df = pd.read_csv("/content/drive/MyDrive/online_retail_II.csv",
                  index_col = "InvoiceDate",
                  parse_dates = True)
display(df)

# There are over 5699 unique items.

df.head()

def wrangle_online_retail2(df):
  '''
  Creates two dataframes from .csv
  Formats all columns to lowercase and replace spaces with underscore.
  Both DataFrames recieve InvoiceDate as index.
  Both DataFrames recieve stockcode_code, country_code, totalspending columns.

  Parameters:
  csv - .csv document specific to Online_Retail2.csv

  Result:
  kndf - "KnowN DataFrame" DataFrame object without Description columns,
  Invoice columns, and rows missing Customer ID.

  undf - "Unknown Data Frame" DataFrame object without Description columns,
  Invoice columns. Excludes Rows with unknown Customer ID.

  Example:
    >>> undf,kndf = wrangle_online_retail2("/content/online_retail_II.csv")
    >>> display(kndf,undf)
  '''

  df.columns = [col.lower().replace(" ", "_") for col in df.columns]
  df.drop(columns=["description", "invoice"], inplace=True)
  # Define the categorical features
  df['stockcode_code'] = OrdinalEncoder().fit_transform(df['stockcode'])
  df['country_code'] = OrdinalEncoder().fit_transform(df['country'])
  ### To reduce complexity and scope, we will eliminiate returns.
  # We assume all returns are negative.
  ### Deal with null(unknown customer_id) values.
  # Create a boolean mask indicating which rows have missing values.
  missing_mask = df.isnull().any(axis=1)
  undf = df[missing_mask]
  kndf = df[~missing_mask]
  # Create totalspending column.
  kndf["totalspending"]  = kndf.loc[:,('quantity')] * kndf.loc[:,('price')]
  return undf,kndf

# the warning: A value is trying to be set on a copy of a slice from a DataFrame.
# is a false positive.
# https://old.reddit.com/r/learnpython/comments/lcmb88/why_am_i_getting_a_settingwithcopywarning_on_this/gm2lgkv/

undf,kndf = wrangle_online_retail2(df)
kndf

kndf.isnull().sum(),undf.isnull().sum()

"""#Method 0: Predict the quantity of the MOST common item, based on orders of the past 30 days.
 Do Ridge Regression first

## Add a rolling function of the average of price variates over the last thirty days.
"""

kndf['stockcode'] = OrdinalEncoder().fit_transform(kndf['stockcode'])
kndf['country'] = OrdinalEncoder().fit_transform(kndf['country'])

minidf = kndf.head(5000)
minidf.head(100)

"""#Method 1: Find Priority Countries by Total Spent.(Done, may need updating)

##Why do some companies have -1000 totalspending?
###if a company bought 1000 of stuff
###then returned 1000 of stuff
###the result should be 0



###Possible Explainations:
  * Items purchused from previous year.
  * Unrecored purchases.
  * Items returned for more than selling price.
"""

### To reduce complexity and scope, we will eliminiate returns. We assume all returns are negative.
print("previous minimum", kndf["totalspending"].min())
kndf = kndf.loc[kndf["totalspending"]>0].reset_index(drop=True)
print("new minimum", kndf["totalspending"].min())

# List top countries by spending.(TODO- test if first three lines does everything)
country_totals = (kndf.groupby("country")
                    ["totalspending"]
                    .sum()
                    .reset_index()
                    .sort_values(by="totalspending",ascending=False)
                    .reset_index(drop=True))
pd.DataFrame(country_totals)
display(country_totals)


"""countries = kndf["country"].unique()
results = []
for i in countries:
  count_spend = kndf['totalspending'].loc[kndf['country']==i]
  results.append(sum(count_spend))
"""

## TEST Alt. code
"""dic = {}
for i in kndf:
  if i['country'] in dic:
    dic[i] = dic[i].value + i['totalspending']
  dic[i] = i['totalspending']

type_ranking = kndf.groupby("country")["totalspending"].sum().reset_index()
type_ranking = type_ranking.sort_values(by="Value", ascending=False).reset_index(drop=True)
"""

Rankings=list(zip(results,countries))
country_totals = pd.DataFrame(Rankings,columns=["total_spending","countries"])

country_totals.sort_values("total_spending",ascending=False).reset_index(drop=True)

"""#Method 2: Code Known Customers by High, Meduim and Low Cart Value(the total sum of all products purchased during an order process.(DONE)

1)Determine your customer segmentation goals and variables.

What are the characteristics of our customers so we can incentivies more sales.

When do they buy? How many? At what price? Language?

2)Break goals into customer-centric segmentation projects.

3)Set up and prioritize each customer segmentation project.

4)Collect and organize your customer data.

5)Segment your customers into groups of your choice.

6)Target and market to your client and user segments.

7)Run regular customer segmentation analysis.

###DataFrame of Known Customers
"""

customer_totals = kndf.groupby("customer_id")["totalspending"].sum().reset_index().sort_values(by="totalspending",ascending=False).reset_index(drop=True)
customer_totals = pd.DataFrame(customer_totals)

fig = plt.figure(figsize = (20, 2))
plt.bar(customer_totals['customer_id'], customer_totals['totalspending'])

plt.xlabel('customer_id')
plt.ylabel('totalspending')
plt.title('customer_totals')
plt.show()

customer_spending = kndf.groupby("customer_id")["totalspending"].sum().reset_index().sort_values(by="totalspending",ascending=False).reset_index(drop=True)
customer_spending["index"] = np.arange(0,len(customer_spending))
customer_spending.set_index("index",inplace=True)

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the data
#customer_spending

# Normalize the data is not needed bc there is only one unit type.(price)
#data_norm = (customer_spending - customer_spending.min()) / (customer_spending.max() - customer_spending.min())

# Choose the number of clusters
k = 2

# Train the K-Means clustering model on the normalized data
kmeans = KMeans(n_clusters=k)
kmeans.fit(customer_spending)

# Visualize the clusters
plt.scatter(customer_spending['totalspending'], range(len(customer_spending)), c=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', s=200, linewidths=3, color='r')
plt.xlabel('totalspending')
plt.ylabel('customer_id')
plt.show()

# Create a list of groups for each cluster
group_labels = kmeans.labels_
group_0 = customer_spending[group_labels == 0]
group_1 = customer_spending[group_labels == 1]

# Print the number of people in each group
print('Number of people in Group 0:', len(group_0))
print('Number of people in Group 1:', len(group_1))

group_1

# Further reserach - How often are these purchases made?

"""#Method 3  Bar Chart of Product Trends By Month (WORKING)

"""

# when do these items sell? Can we predict future sales?
# We are going to create a data frame that ranks items by quantity.

#How often inventory needs to be replenished?
#What are the required stock levels for each SKU?
#How to calculate demand for different selling points?
#How to manage inventory when anomalies occur?

"""ARIMA/SARIMA
Exponential Smoothing
Regression models
Gradient Boosting
Long Short-Term Memory (LSTM)
Ensemble Models
Transformer-based Models"""

type(df.index)

df.head()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.arima_model import ARIMA

# Load the data
#data = pd.read_csv('online_retail_II.csv', parse_dates=['InvoiceDate'])
undf,kndf = wrangle_online_retail2(df)

# A. Twelve lists of most probable item per month in the next year.
# B. List of most popular items for previous months
# C. Interactable graph that displays each item's quantity every month
#     in addition to future months.

# I could use LSTM modesl.
# I could use Time Series with products per week as value.
# I could use Random Forrest with products sold in weeks as value.
# Input Sequence:
#


#
# I could considere factors such as item price, distance to country,
# and Time since last order.
# I could try to find the most likely months given a number of sales.Then reverse
# the process to discover the most likely month. Support Vector Machine

# I could use Logistic regression with product quantitiy as outcome and Time as input.

# If a neural network, what would be the dimensions?
# number of products (additionally time since last similar customer order)

# What would be the trarget?
# Product sould per week
#



# Aggregate the data by month
monthly_data = kndf.groupby(pd.Grouper(freq='M'))
monthly_data = monthly_data['quantity'].sum().reset_index()

# Visualize the data
plt.plot(monthly_data['InvoiceDate'], monthly_data['quantity'],markersize=16)
plt.xlabel('Month')
plt.ylabel('Number of items sold')
plt.xticks(rotation=45)
plt.show()

#For every month in the year provide a dataset listing
# 1) the top 100 items orded for the coming month
# 2) the quantity for these items.

monthly_data.head()

plt.figure(figsize=(10,10))
plt.hist(df["stockcode"].value_counts(), bins=200)
plt.show()

codes=df["stockcode"].unique()
type(codes.tolist())

#top_items=['84077', '85123A', '85099B', '21212', '84879', '22197', '17003', '21977', '84991', '22492', '15036', '21213', '22616', '84755', '20725', '22178', '22386', '21232', '84568', '84946', '21915', '22086', '85099F', '84992', '22952', '71459', '22355', '21982', '20724', '21975', '21980', '21080', '85099C', '23084', '16014', '21931', '21731', '22469', '21790', '22411', '47566', '22151', '21498', '84970S', '21981', '20727', '22423', '21985', '21984', '37410', '22383', '21929', '21181', '20719', '84212', '21928', '20728', '84945', '22417', '20726', '22629', '84978', '20668', '22384', '22382', '84836', '23203', '21137', '21166', '72741', '22356', '21094', '21231', '84692', '20972', '22910', '20971', '20713', '20914', '20723', '21326', '22189', '21175', '22630', '22470', '22585', '84270', '22029', '22693', '22961', '22489', '21497', '84970L', '21791', '21733', '20712', '22077', '22909', '84598', '20975', '22294', '22385', '82482', '79321', '21704', '22659', '16156S', '22090', '85152', '84947', '21500', '22379', '21086', '21121', '22457', '22326', '22551', '21967', '22865', '22560', '21499', '22969', '21210', '21976', '82494L', '21787', '21703', '22045', '21918', '84826', '23199', '22558', '22328', '22577', '40016', '21085', '22297', '21091', '22556', '21930', '22554', '22866', '21986', '22083', '20718', '22867', '23209', '21099', '84347', '21914', '82580', '75049L', '21889', '16161P', '20974', '21877', '22534', '22699', '21092', '23201', '22610', '22740', '21122', '21829', '22595', '22993', '71477', '22188', '85175', '22418', '21495', '20711', '22348', '82600', '82583', '22998', '22114', '22030', '16033', '22149', '22578', '22147', '17084R', '23206', '21891', '23310', '23307', '22555', '84949', '84029E', '84378', '21314', '35961', '22112', '22561', '84988', '21506', '22027', '22632', '21672', '82484', '22951', '21390', '21916', '21385', '20993', '16237', '22444', '21871', '23202', '22666', '22741', '21669', '22557', '20676', '15056N', '23077', '21668', '22467', '84380', '22139', '84997D', '22530', '84050', '21673', '21136', '48138', 'POST', '22536', '20973', '23344', '21124', '22662', '22111', '16161U', '22652', '21108', '22296', '22697', '20685', '21908', '21174', '79000', '22791', '62018', '15056BL', '22727', '22988', '21088', '22544', '22633', '85220', '21238', '22381', '21156', '47591D', '22734', '22759', '84997B', '21671', '22219', '22959', '22646', '22158', '21165', '21868', '22960', '21096', '22333', '15034', '22704', '21509', '21169', '21670', '22065', '23301', '22621', '22295', '37340', '22084', '22440', '23232', '23298', '22907', '22130', '23293', '22966', '21098', '22620', '22531', '21559', '22661', '21507', '35970', '22087', '22335', '23309', '85150', '22367', '23343', '84987', '23076', '21917', '22047', '84997C', '22553', '21989', '21154', '82581', '23230', '84950', '22698', '20677', '21485', '22726', '21892', '48194', '20979', '22041', '21754', '10002', '23204', '22154', '84375', '22464', '21481', '22432', '22771', '21936', '22950', '21786', '22243', '21240', '84029G', '21172', '22198', '22082', '21239', '23207', '48187', '84520B', '22834', '22491', '22816', '22712', '21519', '21479', '22720', '21988', '23231', '22150', '21623', '22579', '22352', '22037', '21523', '22710', '21935', '21242', '22419', '23200', '22609', '22945', '20991', '21900', '21078', '21810', '22488', '84832', '22413', '21974', '47503E', '21508', '21875', '23078', '22161', '21899', '22983', '21313', '72008', '22549', '23300', '82552', '22196', '84406B', '22602', '22064', '22437', '51014A', '22818', '20675', '21535', '85049E', '21874', '21621', '85110', '22138', '21155', '21429', '22046', '22596', '20996', '20983', '22728', '22631', '85049A', '22540', '21870', '22028', '37413', '23208', '23245', '21833', '84596B', '22713', '20992', '20717', '21983', '21811', '47556B', '17096', '18007', '85014B', '22152', '22900', '23308', '23170', '23171', '21901', '22155', '21987', '85199S', '84971S', '23295', '22227', '48185', '82582', '22156', '21544', '37370', '22338', '21164', '85061W', '21830', '22563', '22571', '21592', '22113', '22722', '21746', '22273', '22537', '21035', '22466', '20984', '22986', '22528', '22645', '20679', '47599A', '22663', '22153', '22614', '22567', '22539', '23355', '22608', '21755', '21114', '72351B', '22507', '23205', '22908', '21243', '21876', '47590B', '47503A', '20759', '21843', '20756', '22593', '82578', '16046', '22601', '21844', '20760', '37351', '16047', '22144', '22174', '22024', '22711', '22708', '82551', '21524', '85178', '23167', '22716', '22570', '22569']



# Takes 4m to compute.
codes = df["stockcode"].unique()
codes.tolist()
quantities = []
for i in codes[:-1]:
  x = df['quantity'].loc[df["stockcode"]==i]
  x = sum(x)
  quantities.append(x)
  items = list(zip(codes,quantities))
items = pd.DataFrame(top_items,columns=["stockcode","Number Ordered"])
items

data.columns

data = pd.read_csv('online_retail_II.csv',index_col="invoicedate", parse_dates=['invoicedate'])

data.head()

#seperate by month.
#sum the amount ordered that month.
#reduce to top 100 items.
#sort lowest to highest.

type(data.index[0])

jan_report=data.loc[data.index.month == 1]
feb_report=data.loc[data.index.month == 2]
mar_report=data.loc[data.index.month == 3]
apr_report=data.loc[data.index.month == 4]
may_report=data.loc[data.index.month == 5]
jun_report=data.loc[data.index.month == 6]
jul_report=data.loc[data.index.month == 7]
aug_report=data.loc[data.index.month == 8]
sep_report=data.loc[data.index.month == 9]
oct_report=data.loc[data.index.month == 10]
nov_report=data.loc[data.index.month == 11]
dec_report=data.loc[data.index.month == 12]

import calendar
reports = {}
for month in range(1, 13):
    month_name = calendar.month_name[month]
    reports[month_name] = data.loc[data.index.month == month]

# Concatenate the DataFrames into a single DataFrame
all_reports = pd.concat(reports.values())

# Reset the index of the new DataFrame
all_reports = all_reports.reset_index(drop=False)

### Third attempt to make 12 DataFrame reports

# Assuming data is a DataFrame with an "invoicedate" column
data = pd.read_csv("online_retail_II.csv")
data["invoicedate"] = pd.to_datetime(data["invoicedate"])

# Create twelve different DataFrames, one for each month
reports = {}
for month in range(1, 13):
    month_name = pd.Timestamp(month=month, day=1, year=2000).strftime('%B')
    reports[month_name] = data.loc[data["invoicedate"].dt.month == month]

# Print the first five rows of the January report
reports["January"].head()

#New dataframe
#Get reports by the month
for i in reports

a=reports["January"].groupby("description")["quantity"].sum().sort_values(ascending=False)
a=pd.DataFrame(a)
a.head(100)

#Get top hundred, see which products are trending in a scatterplot visualization.
#This will allow discriptions in x and quanitty in y.

import matplotlib.pyplot as plt
b=a.head(100)
plt.figure(figsize=(20,10))

plt.scatter(x=b.index,y=b["quantity"])
plt.xticks(b.index, rotation=85)
plt.show()

top_items=top_items.sort_values(by="Number Ordered",ascending=False).head(500)

df.groupby(pd.Grouper(key='invoicedate', freq='M'))

x=pd.to_datetime("2009-12-01 07:45:00")
x_list=(x,x,x,x)
b=x_list.groupby(pd.Grouper(key='invoicedate', freq='M'))
b

for i in top_items["stockcode"]:
  x=df.loc[df['stockcode']==i]
    for k in range(len(x)):
      if x[k].month==1
        n.append(1)

plt.figure(figsize=(10,10))
plt.hist(top_items.value_counts(), bins=200)
plt.show()

top_items.sorted(ascending=False).head(500)

df['stockcode'].loc[df["stockcode"].value_counts()>500)]

meth2_df=df[['invoicedate','stockcode','quantity']]
meth2_df.set_index("invoicedate",inplace=True)
meth2_df.head()

type(meth2_df['invoicedate'][0])

meth2_df["Month"]=pd.DatetimeIndex(df["invoicedate"]).month
meth2_df["Month"].value_counts()

meth2_df.loc[df['stockcode']contains ]

stockseries=meth2_df['stockcode']

months=[12,1,2,3,4,5,6,7,8,9,10,11,12,1,2,3,4,5,6,7,8,9,10,11,12]
for i in months:

  for i in stockseries:
    rows=df.loc[df['stockcode']==i]
    date=.to_datetime64()
  date=np.datetime64(date, 'M')
  date + 1
#every step another month is added until the last month is reached.
# The totals are then saved as values to a dictionary with Stockseries.index as the keys.

months = ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
          "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]
df['months'] = pd.Categorical(df['months'], categories=months, ordered=True)
df.sort_values(...)  # same as you have now; can use inplace=True

"""###Deal with Null Values"""



#Found all Missing Customers
MissingCustomers=df.loc[df['customer_id'].isnull()==True]
#Distribution of Quantiy Ordered for all Customers
fig = plt.subplots(figsize =(12, 8))
plt.hist(df['quantity'], bins=100,range=(-20,50))

plt.show()

#Distribution of Quantiy Ordered for all Customers
fig = plt.subplots(figsize =(12, 8))
plt.hist(MissingCustomers['quantity'], bins=100,range=(-20,50))

plt.show()

known_customers=df.loc[df['customer_id'].isnull()==False]

len(known_customers['customer_id'].loc[known_customers['customer_id']==12748.0])

known_customers['customer_id'].value_counts().index()

NonR_customer=[]
for i in df['customer_id'].value_counts().index:
  if len(known_customers['customer_id'].loc[known_customers['customer_id']==i])>1:
    continue
  NonR_customer.append(i)

"""#Method 4:Find Repeat Customers and Perform Random Forest Classifier"""

repeat_customer = []
for i in df['customer_id']:
  if i in nonr_customer:
    repeat_customer.append(False)
    continue
  repeat_customer.append(True)

df['Repeat_Customer']=Repeat_customer
df.columns

"""###TODO:Replace country with Ordinal Encoder
###TODO:Standardize the values(price, quantitiy)
"""

#Predict Repeat Customers Using ____  'stockcode', 'quantity', 'invoicedate', 'price',  'country',
X=df[['stockcode', 'quantity', 'invoicedate', 'price',  'country']]
y=df['Repeat_Customer']

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20)

len(X_test)/len(X_train)

#Encode stockcode
from sklearn.preprocessing import OrdinalEncoder
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier

Pipe=make_pipeline(OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=-1),RandomForestClassifier())
Pipe.fit(X_train,y_train)
#df['stockcode']=Ord.transform(df['stockcode'])
#df['country']=Ord.tranform(df['country'])

print(Pipe.score(X_test,y_test))

from sklearn.metrics import f1_score

f1_score(y_test,Pipe.predict(X_test))

from sklearn.metrics import classification_report

print(classification_report(y_test,Pipe.predict(X_test)))

"""###From the data, we can see that a grand majority of customers are returning customers. Churn is extremly low."""



"""#Method 6: Multiple Linear Regression"""

!pip install pystan~=2.14
!pip install fbprophet

from fbprophet import Prophet

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Set the date column as the index of the dataframe
df.set_index('date', inplace=True)

# Decompose the time series into trend, seasonality, and residuals using statsmodels
decomposition = sm.tsa.seasonal_decompose(df, model='additive')

# Plot the original time series and the decomposed components
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 8))
ax1.plot(df)
ax1.set_title('Original Time Series')
ax2.plot(decomposition.trend)
ax2.set_title('Trend')
ax3.plot(decomposition.seasonal)
ax3.set_title('Seasonality')
ax4.plot(decomposition.resid)
ax4.set_title('Residuals')

# Show the plot
plt.show()



#LEAST ORDINARY SQUARES
# Libraries for R^2 visualization
from ipywidgets import interactive, IntSlider, FloatSlider
from math import floor, ceil
from sklearn.base import BaseEstimator, RegressorMixin

# Libraries for model building
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

from category_encoders import OrdinalEncoder

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

target ='quantity'
X = kndf.drop(columns = target, axis = 1)
y = kndf[target]

X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2,random_state=42)

# Providing the mean of our target we can have a baseline we always work with our train data & leave the test data for the end.
y_pred_baseline = [y_train.mean()] * len(y_train)
baseline_mae = mean_absolute_error(y_train, y_pred_baseline)

print('quantity:', y_train.mean())
print('Baseline MAE:', baseline_mae)

#Fitting
model=LinearRegression()
model.fit(X_train,y_train)

#MAE Accuracy Score
print('Training MAE:', mean_absolute_error(y_train, model.predict(X_train)))
print('Test MAE:', mean_absolute_error(y_test, model.predict(X_test)))

# Training RMSE
print('Training R-Squared:', r2_score(y_train, model.predict(X_train)))

# Test RMSE
print('Test R-Squared:', r2_score(y_test, model.predict(X_test)))

# Coefficient with the most value allways is the absolut value of our coefficient in this case is the [0]
print('The coefficient for Stock Code is:', model.coef_[0])
print('The coefficient for price is:', model.coef_[1])
print('The coefficient for customer_id is:', model.coef_[2])
print('The coefficient for country is:', model.coef_[3])
# Intercept
#print('The intercep for our model is:', model.intercept_)

"""#Ridge Regression"""

# what customer ordered how many items and when?
# create chart that predicts prices accross next year by month
#
# flow map?

# do the orders have more to do with holiday or something else?

"""kndf["avg_price_30d"] = kndf["price"].rolling(window=30).mean()
kndf["avg_price_90d"] = kndf["price"].rolling(window=90).mean()
kndf["avg_price_1y"] = kndf["price"].rolling(window=252).mean()
kndf.dropna(inplace=True)

kndf["avg_quant_30d"] = kndf["quantity"].rolling(window=30).std()
kndf["avg_quant_90d"] = kndf["quantity"].rolling(window=90).std()
kndf["avg_quant_1y"] = kndf["quantity"].rolling(window=252).std()
kndf.dropna(inplace=True)
"""

kndf

#Ridge Regression

target ='quantity'
X = kndf.drop(columns = target, axis = 1)
y = kndf[target]

X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2,random_state=42)

X

# Providing the mean of our target we can have a baseline we always work with our train data & leave the test data for the end.
y_pred_baseline = [y_train.mean()] * len(y_train)
baseline_mae = mean_absolute_error(y_train, y_pred_baseline)
baseline_mae

y_train

from sklearn.model_selection import GridSearchCV

#Fitting
ridge=Ridge()
parameters={'alpha':[1,.5,.25,.10,.001]}
grid=GridSearchCV(ridge,parameters,n_jobs=-1,scoring='neg_mean_absolute_error')

grid.fit(X_train,y_train)

from sklearn.metrics import get_scorer_names
get_scorer_names()

grid.best_params_

grid.best_score_,baseline_mae

a=grid.best_estimator_.coef_
for i,k in enumerate(a):
  print(round(i,9),k)

X

y_pred=grid.predict(X_test)

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
import numpy as np

mae = mean_absolute_error(y_test, y_pred)

# Get coefficients
coefficients = gird.coef_

print("Mean Absolute Error:", mae)
print("Coefficients:", coefficients)

#This model is a wierd predictor.

